---
---

@article{motwani2024malt,
  abbr = {Preprint},
  title={MALT: Improving Reasoning with Multi-Agent LLM Training},
  author={Motwani, Sumeet Ramesh and Smith, Chandler and Das, Rocktim Jyoti and Rybchuk, Markian and Torr, Philip H. S. and Laptev, Ivan and Pizzati, Fabio and Clark, Ronald and de Witt, Christian Schroeder},
  journal={arXiv preprint},
  year={2024},
  month={December},
  url={https://arxiv.org/pdf/2412.01928.pdf},
  selected={true},
  dimensions={true}
}

@article{motwani2024secret,
  abbr = {NeurIPS 2024},
  title={Secret Collusion among AI Agents: Multi-Agent Deception via Steganography},
  author={Motwani, Sumeet Ramesh and Baranchuk, Mikhail and Strohmeier, Martin and Bolina, Vijay and Torr, Philip H. S. and Hammond, Lewis and de Witt, Christian Schroeder},
  abstract={Recent capability increases in large language models (LLMs) open up applications in which groups of communicating generative AI agents solve joint tasks. This poses privacy and security challenges concerning the unauthorised sharing of information, or other unwanted forms of agent coordination. Modern steganographic techniques could render such dynamics hard to detect. In this paper, we comprehensively formalise the problem of secret collusion in systems of generative AI agents by drawing on relevant concepts from both the AI and security literature. We study incentives for the use of steganography, and propose a variety of mitigation measures. Our investigations result in a model evaluation framework that systematically tests capabilities required for various forms of secret collusion. We provide extensive empirical results across a range of contemporary LLMs. While the steganographic capabilities of current models remain limited, GPT-4 displays a capability jump suggesting the need for continuous monitoring of steganographic frontier model capabilities. We conclude by laying out a comprehensive research program to mitigate future risks of collusion between generative AI models.},
  journal={Thirty-Eighth Conference on Neural Information Processing Systems},
  year={2024},
  month={February},
  url={https://arxiv.org/pdf/2402.07510.pdf},
  selected={true},
  dimensions={true}
}

@article{putta2024agent,
  abbr = {Preprint},
  title={Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents},
  author={Putta, Pranav and Mills, Edmund and Garg, Naman and Motwani, Sumeet and Finn, Chelsea and Garg, Divyansh and Rafailov, Rafael},
  journal={arXiv preprint},
  year={2024},
  month={August},
  url={https://arxiv.org/pdf/2408.07199.pdf},
  selected={true},
  dimensions={true}
}

@article{draguns2024unelicitable,
  abbr = {NeurIPS 2024},
  title={Unelicitable Backdoors in Language Models via Cryptographic Transformer Circuits},
  author={Draguns, Andis and Gritsevskiy, Andrew and Motwani, Sumeet Ramesh and Rogers-Smith, Charlie and Ladish, Jeffrey and de Witt, Christian Schroeder},
  journal={Thirty-Eighth Conference on Neural Information Processing Systems},
  year={2024},
  month={June},
  url={https://arxiv.org/pdf/2406.02619.pdf},
  selected={true},
  dimensions={true}
}

@article{anwar2024foundational,
  abbr = {TMLR},
  title={Foundational Challenges in Assuring Alignment and Safety of Large Language Models},
  author={Anwar, Usman and Saparov, Abulhair and Rando, Javier and Paleka, Daniel and Turpin, Miles and Hase, Peter and Lubana, Ekdeep Singh and Jenner, Erik and Casper, Stephen and Sourbut, Oliver and Edelman, Benjamin L. and Zhang, Zhaowei and Günther, Mario and Korinek, Anton and Hernandez-Orallo, Jose and Hammond, Lewis and Bigelow, Eric and Pan, Alexander and Langosco, Lauro and Korbak, Tomasz and Zhang, Heidi and Zhong, Ruiqi and hÉigeartaigh, Seán Ó and Recchia, Gabriel and Corsi, Giulio and Chan, Alan and Anderljung, Markus and Edwards, Lilian and Petrov, Aleksandar and de Witt, Christian Schroeder and Motwani, Sumeet Ramesh and Bengio, Yoshua and Chen, Danqi and Torr, Philip H. S. and Albanie, Samuel and Maharaj, Tegan and Foerster, Jakob and Tramer, Florian and He, He and Kasirzadeh, Atoosa and Choi, Yejin and Krueger, David},
  journal={Transactions on Machine Learning Research},
  year={2024},
  month={April},
  url={https://arxiv.org/pdf/2404.09932.pdf},
  selected={true},
  dimensions={true}
}

@article{arXiv:2309.15257,
  abbr = {ICLR 2024},
  title={STARC: A General Framework For Quantifying Differences Between Reward Functions},
  author={Skalse, J. and Farnik, L. and Motwani, S. R. and Jenner, E. and Gleave, A. and Abate, A.},
  abstract={In order to solve a task using reinforcement learning, it is necessary to first formalise the goal of that task as a reward function. However, for many real-world tasks, it is very difficult to manually specify a reward function that never incentivises undesirable behaviour. As a result, it is increasingly popular to use reward learning algorithms, which attempt to learn a reward function from data. However, the theoretical foundations of reward learning are not yet well-developed. In particular, it is typically not known when a given reward learning algorithm with high probability will learn a reward function that is safe to optimise. This means that reward learning algorithms generally must be evaluated empirically, which is expensive, and that their failure modes are difficult to anticipate in advance. One of the roadblocks to deriving better theoretical guarantees is the lack of good methods for quantifying the difference between reward functions. In this paper we provide a solution to this problem, in the form of a class of pseudometrics on the space of all reward functions that we call STARC (STAndardised Reward Comparison) metrics. We show that STARC metrics induce both an upper and a lower bound on worst-case regret, which implies that our metrics are tight, and that any metric with the same properties must be bilipschitz equivalent to ours. Moreover, we also identify a number of issues with reward metrics proposed by earlier works. Finally, we evaluate our metrics empirically, to demonstrate their practical efficacy. STARC metrics can be used to make both theoretical and empirical analysis of reward learning algorithms both easier and more principled.},
  journal={The Twelfth International Conference on Learning Representations},
  year={2023},
  month={September},
  url={https://arxiv.org/pdf/2309.15257.pdf},
  selected={true},
  dimensions={true}
}

@inproceedings{NeurIPS:MASEC:2023,
  abbr = {NeurIPS MASEC},
  title={A Perfect Collusion Benchmark: How can AI agents be prevented from colluding with information-theoretic undetectability?},
  author={Motwani, S. R. and Baranchuk, M. and Hammond, L. and de Witt, C. S.},
  booktitle={Multi-Agent Security Workshop, NeurIPS'23},
  year={2023},
  month={October},
  url={https://openreview.net/pdf?id=FXZFrOvIoc},
  abstract={Secret collusion among advanced AI agents is widely considered a significant risk to AI safety. In this paper, we investigate whether LLM agents can learn to collude undetectably through hiding secret messages in their overt communications. To this end, we implement a variant of Simmon’s prisoner problem using LLM agents and turn it into a stegosystem by leveraging recent advances in perfectly secure steganography. We suggest that our resulting benchmark environment can be used to investigate how easily LLM agents can learn to use perfectly secure steganography tools, and how secret collusion between agents can be countered pre-emptively through paraphrasing attacks on communication channels. Our work yields unprecedented empirical insight into the question of whether advanced AI agents may be able to collude unnoticed.},
  selected={true},
  dimensions={true}
}



