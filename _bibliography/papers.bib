---
---

@article{motwani2024malt,
  abbr = {Preprint},
  title={MALT: Improving Reasoning with Multi-Agent LLM Training},
  author={Motwani, Sumeet Ramesh and Smith, Chandler and Das, Rocktim Jyoti and Rybchuk, Markian and Torr, Philip H. S. and Laptev, Ivan and Pizzati, Fabio and Clark, Ronald and de Witt, Christian Schroeder},
  abstract={Enabling effective collaboration among LLMs is a crucial step toward developing autonomous systems capable of solving complex problems. While LLMs are typically used as single-model generators, where humans critique and refine their outputs, the potential for jointly-trained collaborative models remains largely unexplored. Despite promising results in multi-agent communication and debate settings, little progress has been made in training models to work together on tasks. In this paper, we present a first step toward "Multi-agent LLM training" (MALT) on reasoning problems. Our approach employs a sequential multi-agent setup with heterogeneous LLMs assigned specialized roles: a generator, verifier, and refinement model iteratively solving problems. We propose a trajectory-expansion-based synthetic data generation process and a credit assignment strategy driven by joint outcome based rewards. This enables our post-training setup to utilize both positive and negative trajectories to autonomously improve each model's specialized capabilities as part of a joint sequential system. We evaluate our approach across MATH, GSM8k, and CQA, where MALT on Llama 3.1 8B models achieves relative improvements of 14.14%, 7.12%, and 9.40% respectively over the same baseline model. This demonstrates an early advance in multi-agent cooperative capabilities for performance on mathematical and common sense reasoning questions. More generally, our work provides a concrete direction for research around multi-agent LLM training approaches.},
  journal={arXiv preprint},
  year={2024},
  month={December},
  url={https://arxiv.org/pdf/2412.01928.pdf},
  selected={true},
  dimensions={true}
}

@article{motwani2024secret,
  abbr = {NeurIPS 2024},
  title={Secret Collusion among AI Agents: Multi-Agent Deception via Steganography},
  author={Motwani, Sumeet Ramesh and Baranchuk, Mikhail and Strohmeier, Martin and Bolina, Vijay and Torr, Philip H. S. and Hammond, Lewis and de Witt, Christian Schroeder},
  abstract={Recent capability increases in large language models (LLMs) open up applications in which groups of communicating generative AI agents solve joint tasks. This poses privacy and security challenges concerning the unauthorised sharing of information, or other unwanted forms of agent coordination. Modern steganographic techniques could render such dynamics hard to detect. In this paper, we comprehensively formalise the problem of secret collusion in systems of generative AI agents by drawing on relevant concepts from both the AI and security literature. We study incentives for the use of steganography, and propose a variety of mitigation measures. Our investigations result in a model evaluation framework that systematically tests capabilities required for various forms of secret collusion. We provide extensive empirical results across a range of contemporary LLMs. While the steganographic capabilities of current models remain limited, GPT-4 displays a capability jump suggesting the need for continuous monitoring of steganographic frontier model capabilities. We conclude by laying out a comprehensive research program to mitigate future risks of collusion between generative AI models.},
  journal={Thirty-Eighth Conference on Neural Information Processing Systems},
  year={2024},
  month={February},
  url={https://arxiv.org/pdf/2402.07510.pdf},
  selected={true},
  dimensions={true}
}

@article{putta2024agent,
  abbr = {Preprint},
  title={Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents},
  author={Putta, Pranav and Mills, Edmund and Garg, Naman and Motwani, Sumeet and Finn, Chelsea and Garg, Divyansh and Rafailov, Rafael},
  abstract={Large Language Models (LLMs) have shown remarkable capabilities in natural language tasks requiring complex reasoning, yet their application in agentic, multi-step reasoning within interactive environments remains a difficult challenge. Traditional supervised pre-training on static datasets falls short in enabling autonomous agent capabilities needed to perform complex decision-making in dynamic settings like web navigation. Previous attempts to bridge this ga-through supervised fine-tuning on curated expert demonstrations-often suffer from compounding errors and limited exploration data, resulting in sub-optimal policy outcomes. To overcome these challenges, we propose a framework that combines guided Monte Carlo Tree Search (MCTS) search with a self-critique mechanism and iterative fine-tuning on agent interactions using an off-policy variant of the Direct Preference Optimization (DPO) algorithm. Our method allows LLM agents to learn effectively from both successful and unsuccessful trajectories, thereby improving their generalization in complex, multi-step reasoning tasks. We validate our approach in the WebShop environment-a simulated e-commerce platform where it consistently outperforms behavior cloning and reinforced fine-tuning baseline, and beats average human performance when equipped with the capability to do online search. In real-world booking scenarios, our methodology boosts Llama-3 70B model's zero-shot performance from 18.6% to 81.7% success rate (a 340% relative increase) after a single day of data collection and further to 95.4% with online search. We believe this represents a substantial leap forward in the capabilities of autonomous agents, paving the way for more sophisticated and reliable decision-making in real-world settings.},
  journal={arXiv preprint},
  year={2024},
  month={August},
  url={https://arxiv.org/pdf/2408.07199.pdf},
  selected={true},
  dimensions={true}
}

@article{draguns2024unelicitable,
  abbr = {NeurIPS 2024},
  title={Unelicitable Backdoors in Language Models via Cryptographic Transformer Circuits},
  author={Draguns, Andis and Gritsevskiy, Andrew and Motwani, Sumeet Ramesh and Rogers-Smith, Charlie and Ladish, Jeffrey and de Witt, Christian Schroeder},
  abstract={The rapid proliferation of open-source language models significantly increases the risks of downstream backdoor attacks. These backdoors can introduce dangerous behaviours during model deployment and can evade detection by conventional cybersecurity monitoring systems. In this paper, we introduce a novel class of backdoors in autoregressive transformer models, that, in contrast to prior art, are unelicitable in nature. Unelicitability prevents the defender from triggering the backdoor, making it impossible to evaluate or detect ahead of deployment even if given full white-box access and using automated techniques, such as red-teaming or certain formal verification methods. We show that our novel construction is not only unelicitable thanks to using cryptographic techniques, but also has favourable robustness properties. We confirm these properties in empirical investigations, and provide evidence that our backdoors can withstand state-of-the-art mitigation strategies. Additionally, we expand on previous work by showing that our universal backdoors, while not completely undetectable in white-box settings, can be harder to detect than some existing designs. By demonstrating the feasibility of seamlessly integrating backdoors into transformer models, this paper fundamentally questions the efficacy of pre-deployment detection strategies. This offers new insights into the offence-defence balance in AI safety and security.},
  journal={Thirty-Eighth Conference on Neural Information Processing Systems},
  year={2024},
  month={June},
  url={https://arxiv.org/pdf/2406.02619.pdf},
  selected={true},
  dimensions={true}
}

@article{anwar2024foundational,
  abbr = {TMLR},
  title={Foundational Challenges in Assuring Alignment and Safety of Large Language Models},
  author={Anwar, Usman and Saparov, Abulhair and Rando, Javier and Paleka, Daniel and Turpin, Miles and Hase, Peter and Lubana, Ekdeep Singh and Jenner, Erik and Casper, Stephen and Sourbut, Oliver and Edelman, Benjamin L. and Zhang, Zhaowei and Günther, Mario and Korinek, Anton and Hernandez-Orallo, Jose and Hammond, Lewis and Bigelow, Eric and Pan, Alexander and Langosco, Lauro and Korbak, Tomasz and Zhang, Heidi and Zhong, Ruiqi and hÉigeartaigh, Seán Ó and Recchia, Gabriel and Corsi, Giulio and Chan, Alan and Anderljung, Markus and Edwards, Lilian and Petrov, Aleksandar and de Witt, Christian Schroeder and Motwani, Sumeet Ramesh and Bengio, Yoshua and Chen, Danqi and Torr, Philip H. S. and Albanie, Samuel and Maharaj, Tegan and Foerster, Jakob and Tramer, Florian and He, He and Kasirzadeh, Atoosa and Choi, Yejin and Krueger, David},
  abstract={This work identifies 18 foundational challenges in assuring the alignment and safety of large language models (LLMs). These challenges are organized into three different categories: scientific understanding of LLMs, development and deployment methods, and sociotechnical challenges. Based on the identified challenges, we pose 200+ concrete research questions.},
  journal={Transactions on Machine Learning Research},
  year={2024},
  month={April},
  url={https://arxiv.org/pdf/2404.09932.pdf},
  selected={true},
  dimensions={true}
}

@article{arXiv:2309.15257,
  abbr = {ICLR 2024},
  title={STARC: A General Framework For Quantifying Differences Between Reward Functions},
  author={Skalse, J. and Farnik, L. and Motwani, Sumeet Ramesh and Jenner, E. and Gleave, A. and Abate, A.},
  abstract={In order to solve a task using reinforcement learning, it is necessary to first formalise the goal of that task as a reward function. However, for many real-world tasks, it is very difficult to manually specify a reward function that never incentivises undesirable behaviour. As a result, it is increasingly popular to use reward learning algorithms, which attempt to learn a reward function from data. However, the theoretical foundations of reward learning are not yet well-developed. In particular, it is typically not known when a given reward learning algorithm with high probability will learn a reward function that is safe to optimise. This means that reward learning algorithms generally must be evaluated empirically, which is expensive, and that their failure modes are difficult to anticipate in advance. One of the roadblocks to deriving better theoretical guarantees is the lack of good methods for quantifying the difference between reward functions. In this paper we provide a solution to this problem, in the form of a class of pseudometrics on the space of all reward functions that we call STARC (STAndardised Reward Comparison) metrics. We show that STARC metrics induce both an upper and a lower bound on worst-case regret, which implies that our metrics are tight, and that any metric with the same properties must be bilipschitz equivalent to ours. Moreover, we also identify a number of issues with reward metrics proposed by earlier works. Finally, we evaluate our metrics empirically, to demonstrate their practical efficacy. STARC metrics can be used to make both theoretical and empirical analysis of reward learning algorithms both easier and more principled.},
  journal={The Twelfth International Conference on Learning Representations},
  year={2023},
  month={September},
  url={https://arxiv.org/pdf/2309.15257.pdf},
  selected={true},
  dimensions={true}
}

@inproceedings{NeurIPS:MASEC:2023,
  abbr = {NeurIPS MASEC},
  title={A Perfect Collusion Benchmark: How can AI agents be prevented from colluding with information-theoretic undetectability?},
  author={Motwani, Sumeet Ramesh and Baranchuk, M. and Hammond, L. and de Witt, C. S.},
  booktitle={Multi-Agent Security Workshop, NeurIPS'23},
  year={2023},
  month={October},
  url={https://openreview.net/pdf?id=FXZFrOvIoc},
  abstract={Secret collusion among advanced AI agents is widely considered a significant risk to AI safety. In this paper, we investigate whether LLM agents can learn to collude undetectably through hiding secret messages in their overt communications. To this end, we implement a variant of Simmon’s prisoner problem using LLM agents and turn it into a stegosystem by leveraging recent advances in perfectly secure steganography. We suggest that our resulting benchmark environment can be used to investigate how easily LLM agents can learn to use perfectly secure steganography tools, and how secret collusion between agents can be countered pre-emptively through paraphrasing attacks on communication channels. Our work yields unprecedented empirical insight into the question of whether advanced AI agents may be able to collude unnoticed.},
  selected={true},
  dimensions={true}
}



